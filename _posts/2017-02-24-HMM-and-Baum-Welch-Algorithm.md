---
layout: post
title:  "HMM and Baum Welch Algorithm"
categories: Machine Learning
tags:  algorithms, Machine Learning
author: Ziwei Zhu
---

* content
{:toc}


This article is for the lab seminar. I gave a presentation about the paper ' [Fusion of Inertial and Depth Sensor Data for Robust Gesture Recognition](https://www.dropbox.com/s/imwdujgf4kdat04/Fusion%20of%20Inertial%20and%20Depth%20Sensor%20Data%20for%20Robust%20Hand%20Gesture%20Recognition.pdf?dl=0)'. The most important part of this paper is Hidden Markov Model and the algorithm of training the model which named Baum Welch algorithm. Here I will explain them.


## Hidden Markov Model (HMM) Definition

A Hidden Markov Model is a probabilistic model of the joint probability of a collection of random variables {O1,â€¦OT, Q1,â€¦QT}. The Ot variables are discrete observations and the Qt variables are â€œhiddenâ€ and discrete. 

1. Qt is a discrete random variable with N possible values {1â€¦.N}.
2. The observation is one of L possible observation symbols, Ot âˆˆ { o1 . . . oL }. 
3. Transition matrix A = {aij} = P(Qt = j | Qt-1 = i}, is an N by N matrix.
4. The special case of time t = 1 is described by the initial state distribution Ï€i = P(Q1 = i).
5. The probability of a particular observation vector at a particular time t for state j is described by: 
<p align='center'>bj(Ot) = P(Ot = ot | Qt = j). (B={bij} is an L by N matrix).</p>

Therefore, we can describe a HMM by: Î» = (A,B,Ï€ ).
![](http://gekkoquant.com/wp-content/uploads/2014/05/hidden-markov-model.png)


## Conditional Independence Assumptions
![](https://github.com/Zziwei/Zziwei.github.io/blob/master/_resources/2017-02-24-HMM-and-Baum-Welch-Algorithm/conditional%20independence%20assumptions.PNG)

## Model Training
![](https://github.com/Zziwei/Zziwei.github.io/blob/master/_resources/2017-02-24-HMM-and-Baum-Welch-Algorithm/model%20training%20objective%20formular.PNG)
that is, find the HMM Î», that maximizes the probability of the observation sequence O.

### Two probabilities for training

The probability of the t-th state Qt = i, given the observation sequence O and model Î».
Compute P(Qt = i | O, Î») denoted as ğ›¾ğ‘–(ğ‘¡).

The probability of the t-th state Qt = i and t+1th state Qt+1 = j, given the observation sequence O and model Î».
Compute P(Qt = i, Qt+1 = j | O, Î») denoted as ğœ‰ğ‘–ğ‘—(ğ‘¡).

## Forward Algorithm
Compute P(Qt = i, O1â€¦t | Î»), denoted as Î±ğ‘–(ğ‘¡).
The probability of seeing the partial sequence O1â€¦Ot and the tth state Qt = i, given the model Î»

![](https://github.com/Zziwei/Zziwei.github.io/blob/master/_resources/2017-02-24-HMM-and-Baum-Welch-Algorithm/forward%20alg..PNG)

## Backward Algorithm
Compute P(Ot + 1 â€¦ T | Qt = i, Î»), Marked as ğ›½ğ‘–(ğ‘¡).
The probability of seeing the ending partial sequence Ot + 1â€¦OT , given the tth state Qt = i and the model Î».

![](https://github.com/Zziwei/Zziwei.github.io/blob/master/_resources/2017-02-24-HMM-and-Baum-Welch-Algorithm/backward%20alg..PNG)

## Baum Welch Algorithm
![](https://github.com/Zziwei/Zziwei.github.io/blob/master/_resources/2017-02-24-HMM-and-Baum-Welch-Algorithm/baum%20welch%20alg..PNG)

## Reference

- [Hidden Markov Model From Wikipedia](https://en.wikipedia.org/wiki/Hidden_Markov_model)
- [The Baumâ€“Welch algorithm for estimating a Hidden Markov Model](http://www.ph.biu.ac.il/faculty/kanter/BW.pdf)